# 零样本学习

在介绍性章节之后，我们将详细解释零样本学习（ZSL）。本章旨在涵盖：
- 各类 ZSL 的定义及其差异。
- 一个使用语义嵌入的 ZSL 深入示例 [1]。

## 零样本学习 vs. 广义零样本学习

零样本学习和广义零样本学习（GZSL）属于机器学习算法类型，其中图像分类模型需要对训练中未包含的标签进行分类。ZSL 和 GZSL 非常相似，主要区别在于模型的评估方式 [2]。

对于 ZSL，模型仅评估其分类未见类别图像的能力——ZSL 测试数据集仅包含未见类别的观测数据。对于 GZSL，模型在已见和未见类别上进行评估——这更接近于真实应用场景。总体而言，GZSL 更具挑战性，因为模型需要确定某个观测数据是属于新类别还是已知类别。

## 归纳零样本学习 vs. 传导零样本学习

根据训练数据的类型，零样本学习可以分为两类：

在归纳 ZSL 中，模型仅在包含已见类别的数据集上进行训练，未见类别的数据不参与训练。学习过程侧重于从训练数据中提取并推广模式，然后将其应用于未见类别的分类。该方法假设训练期间已见和未见数据之间的明显分离，强调模型从训练数据推广到未见类别的能力。

传导 ZSL 不同之处在于允许模型在训练期间访问部分关于未见类别的信息，通常是未见类别的属性或未标记示例，但不包括标签。这种方法利用未见数据的结构性信息来训练一个更具泛化能力的模型。

在接下来的章节中，我们将基于 Google 的一篇经典研究论文的主要概念 [1]，给出归纳 ZSL 的示例。

## 使用语义嵌入的零样本学习示例

如前一章所述，开发成功的 ZSL 模型不仅需要图像和类别标签。仅依靠图像几乎不可能对未见类别进行分类。ZSL 利用辅助信息（如语义属性或嵌入）来帮助对未见类别的图像进行分类。在详细说明之前，以下是对语义嵌入的简要介绍，适合不熟悉此术语的读者。

### 什么是语义嵌入？

语义嵌入是语义信息的向量表示，承载了数据的意义和解释。例如，通过语音文本传达的信息是一种语义信息。语义信息不仅包含单词或句子的直接含义，还包括上下文和文化含义。

嵌入是将语义信息映射到实数向量的过程。语义嵌入通常通过无监督机器学习模型（如 Word2Vec [3] 或 GloVe [4]）学习。所有类型的文本信息（如单词、短语或句子）都可以基于设定的程序转化为数值向量。语义嵌入在高维空间中描述单词，其中单词之间的距离和方向反映了它们的语义关系。这使机器能够通过对单词嵌入的数学操作来理解单词的用法、同义词和上下文。

### 利用语义嵌入实现零样本学习

在训练过程中，ZSL 模型学习将已见类别图像的视觉特征与相应的语义嵌入联系起来。目标是最小化图像的投影视觉特征与其类别的语义嵌入之间的距离。该过程帮助模型学习图像与语义信息之间的对应关系。

由于模型已经学习将图像特征投影到语义空间，因此它可以尝试通过将未见类别图像的图像特征投影到相同空间并将其与未见类别的嵌入进行比较来对未见类别图像进行分类。对于未见类别图像，模型计算其投影嵌入，然后搜索未见类别中距离最近的语义嵌入。最近的嵌入对应的未见类别即为图像的预测标签。

总之，语义嵌入在 ZSL 中起到了核心作用，使模型能够扩展其分类能力。这种方法提供了一种更灵活和可扩展的方式来分类大量的现实世界类别，而不需要标记数据集。

## 与 CLIP 的比较

ZSL 与 CLIP（对比语言-图像预训练）[5] 之间的关系源于其共同目标，即使模型能够识别和分类训练数据中未出现的类别的图像。然而，CLIP 代表了一种显著的进步，并更广泛地应用了 ZSL 的基本原理，采用了一种全新的学习和泛化方法。

CLIP 和 ZSL 的关系可以描述为：

- ZSL 和 CLIP 都旨在将图像分类到训练期间未见的类别中。然而，传统 ZSL 方法可能依赖于预定义的语义嵌入或属性来填补已见类别与未见类别之间的差距，而 CLIP 直接从自然语言描述中学习，从而可以在不需要特定任务嵌入的情况下推广到更广泛的任务。

- CLIP 是多模态学习的一个典型示例，模型从文本和视觉数据中学习。这种方法与 ZSL 中使用辅助信息以提高分类性能的理念一致。CLIP 进一步直接从原始文本和图像中学习，使其能够理解和表示视觉内容和描述性语言之间的关系。

## 零样本学习的评估数据集

每年都会提出新的 ZSL 方法，导致由于评估方法的不同而难以识别出最佳方法。标准化的评估框架和数据集被优先用于评估不同的 ZSL 方法。一项关于经典 ZSL 方法的对比研究见于 [6]。常用的 ZSL 评估数据集包括：

- **具有属性的动物数据集（AwA）**

该数据集用于评估迁移学习算法，尤其是基于属性的分类 [7]。它包含 50 个动物类别的 30475 张图像，每张图像有六种特征表示。

- **加州理工-加州大学圣地亚哥分校鸟类数据集 200-2011（CUB）**

用于细粒度视觉分类任务的数据集。包含 200 个鸟类子类别的 11788 张图像。每张图像都有 1 个子类别标签、15 个部位位置、312 个二进制属性和 1 个边界框。此外，通过亚马逊的 Mechanical Turk 收集了每张图像的十句描述，并精心构造这些描述以不包含任何子类别信息。

- **Sun 数据库（SUN）**

第一个大规模场景属性数据库。数据集包含 899 个类别的 130519 张图像，可用于高级场景理解和细粒度场景识别。

- **Attribute Pascal 和 Yahoo 数据集（aPY）**

一个粗粒度数据集，由来自 3 个广泛类别（动物、物体和车辆）的 15339 张图像组成，进一步分为共计 32 个子类别。

- **ILSVRC2012/ILSVRC2010（ImNet-2）**

ImageNet 大规模视觉识别挑战赛（ILSVRC）评估大规模物体检测和图像分类算法 [8]。

## 参考文献

- [1] Frome et al., DeViSE: A Deep Visual Semantic Embedding Model, NIPS, (2013)
- [2] Pourpanah et al., A Review of Generalized Zero-Shot Learning Methods (2022).
- [3] Mikilov et al., Efficient Estimation of Word-Representations in Vector Space, ICLR (2013). 
- [4] Pennington et al., Glove: Global Vectors for Word Representation, EMNLP (2014).
- [5] Radford et al., Learning Transferable Visual Models From Natural Language Supervision, arXiv (2021).
- [6] Xian et al., Zero-Shot Learning - The Good, the Bad and the Ugly, CVPR (2017).
- [7] Lampert et al., Learning to Detect Unseen Object Classes by Between-Class Attribute Transfer, CVPR (2009).
- [8] Deng et al., Imagenet: A Large-Scale Hierarchical Image Database, CVPR (2012).