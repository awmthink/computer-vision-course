
# 基于多模态的视频模型

如前面章节所讨论的，视频可以简单地定义为一系列图像。然而，与单一图像不同，视频包含了多种模态，如声音、文本和运动。从这个角度来看，为了正确理解视频，我们必须同时考虑多种模态。在本章中，我们首先简要解释视频中可能存在的模态类型。接着，我们介绍一些可以通过对齐视频与不同模态来学习的架构。

## 视频中存在哪些模态？

视频不仅仅是图像序列，它还包含多种模态。理解这些不同的模态对于全面的视频分析和处理至关重要。视频中主要存在的模态包括：

1. 视觉模态（帧/图像）：最常见的模态，由一系列图像组成，提供视频的视觉信息。
2. 音频模态（声音）：包括对话、背景音乐和环境声音，可以传达有关视频背景的上下文信息。
3. 文本模态（字幕/标注）：作为字幕、标注或屏幕上的文字出现，提供与视频内容相关的明确的信息。
4. 运动模态（动态变化）：捕捉视频帧之间的时间变化，反映运动和过渡。
5. 深度模态：表示视频的三维空间信息。
6. 传感器模态：在某些应用中，视频可能包括温度或生物识别数据等模态。

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/Multimodal_Based_Video_Models/Modality_example.jpg" alt="模态示例。图像来自原始的 LanguageBind 论文">
</div>

除了上述提到的模态，视频还可以包含更多种类的模态。请确保考虑哪些模态对于您的具体工作或项目是必要的。在下一节中，我们将探讨能够联合表示和对齐这些模态的视频架构。

## 视频与文本

### VideoBERT

**概述**

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/Multimodal_Based_Video_Models/Overview_VideoBERT.png" alt="VideoBERT 模型架构">
</div>

[VideoBERT](https://arxiv.org/abs/1904.01766) 是一个将 BERT 架构直接应用于视频数据的尝试。与语言模型中的 BERT 类似，VideoBERT 的目标是通过无监督学习获得良好的视觉-语言表示。对于文本模态，VideoBERT 使用 ASR（自动语音识别）将音频转换为文本，然后获得 BERT 的 token 嵌入。对于视频，它使用 S3D 提取每帧的 token 嵌入。

**关键特性**

1. **语言-视觉对齐**：分类给定文本和视频帧是否对齐。
2. **掩码语言建模**：预测文本中的掩码 token（与 BERT 相同）。
3. **掩码帧建模**：预测被掩盖的视频帧（类似于 MLM 在文本中预测掩盖的 token）。

**意义所在**

VideoBERT 是首个通过学习联合表示有效整合视频与语言理解的模型之一。与之前的方法不同，VideoBERT 不使用检测模型进行图像-文本标注。相反，它使用 *聚类算法* 来实现掩码帧建模，使得模型能够在不需要显式标注数据的情况下预测掩盖的帧。

### MERLOT

**概述**

[MERLOT](https://arxiv.org/abs/2106.02636) 旨在通过从大规模视频-文本数据集中学习来改善多模态推理。它专注于理解视觉和文本信息之间的相互作用，并且不依赖于标注数据。通过利用大规模未标注数据集 **YT-Temporal-180M**，**MERLOT** 展示了在视觉常识推理方面的强大性能，而无需依赖大量的视觉监督。

**关键特性**

1. 时间重排序任务（来自 [HERO](https://aclanthology.org/2020.emnlp-main.161.pdf)）
2. 帧-字幕匹配任务（来自 [CBT](https://arxiv.org/pdf/1906.05743)，[HAMMER](https://aclanthology.org/2020.emnlp-main.161.pdf)）
3. 掩码语言建模

**意义所在**

尽管模型架构和训练方法并不全新，MERLOT 通过在 **YT-Temporal-180M** 上训练，从而实现了性能的提升。这个大规模的数据集使得模型能够更好地理解时间动态和多模态交互，从而增强了在视频-语言任务中的推理和预测能力。

<u>注</u>：如果您想了解 MERLOT 的详细训练过程，请务必参考 MERLOT 论文以及早期的工作，如 [HERO](https://aclanthology.org/2020.emnlp-main.161.pdf)，[CBT](https://arxiv.org/pdf/1906.05743) 和 [HAMMER](https://aclanthology.org/2020.emnlp-main.161.pdf)。

## 视频与音频、文本

### VATT（视觉-音频-文本 Transformer）

**概述**

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/Multimodal_Based_Video_Models/Overview_VATT.png" alt="VATT 模型架构">
</div>

[VATT](https://arxiv.org/abs/2104.11178) 是一个设计用于从原始视频、音频和文本中进行自监督学习的模型。对每种模态应用了不同的 token 化和位置编码方法，VATT 使用 Transformer 编码器有效地整合来自原始多模态数据的表示。因此，它在动作识别、文本到视频检索等多个下游任务中取得了强大的性能。

**关键特性**

1. 模态特定和模态无关：**模态特定**版本为每种模态使用独立的 Transformer 编码器，而模态无关版本则使用一个单一的 Transformer 编码器来整合所有模态。尽管模态特定版本的表现更好，但**模态无关**版本在下游任务中仍表现出强劲的性能，且参数较少。
2. Droptoken：由于视频中包含冗余信息（音频和文本数据），仅抽取部分 token 可以实现更高效的训练。
3. 多模态对比学习：对于视频-音频对，使用了噪声对比估计（NCE）；对于视频-文本对，应用了多实例学习 NCE（MIL-NCE）。

**意义所在**

以往使用 Transformer 进行视频多模态任务的模型通常过度依赖视觉数据，且需要大量的训练时间和计算复杂度。相比之下，VATT 利用 **Droptoken** 和 **权重共享**，从原始视觉、音频和文本数据中学习强大的多模态表示，且计算复杂度相对较低。

### Video-Llama

**概述**

[Video-LLaMA](https://arxiv.org/abs/2306.02858) 是一个旨在扩展大规模语言模型（LLM）以理解视频中的视觉和听觉内容的多模态框架。它整合了视频、音频和文本，使得模型能够处理并生成基于视听信息的有意义的响应。Video-LLaMA 解决了两个关键挑战：捕捉视觉场景中的时间变化，以及将音频-视觉信号整合到统一的系统中。

**关键特性**

Video-LLaMA 具有两个分支：

1. <u>视觉-语言分支</u> 用于处理视频帧。
2. <u>音频-语言分支</u> 用于处理音频信号。

这两个分支分别进行训练，经历了预训练和微调两个阶段。在预训练阶段，模型学习如何整合不同的模态，而在微调阶段，它专注于提高准确执行指令的能力。

对于视觉-语言分支，有丰富的视觉-文本数据可用。然而，对于音频-语言分支，缺乏足够的音频-文本数据。为了解决这个问题，模型利用 **ImageBind**，使音频-语言分支可以使用视觉-文本数据进行训练。

**意义所在**

以往的模型在处理视觉和听觉内容时存在困难。Video-LLaMA 通过将这些模态整合到一个框架中来解决这个问题，捕捉视频中的时间变化，并对齐音频-视觉信号。它通过使用跨模态预训练和指令微调，克服了早期研究的局限性，在基于视频的对话等多模态任务中取得了强劲的表现，且无需依赖于单独的模型。

## 视频和多模态

### ImageBind

**概述**

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/Multimodal_Based_Video_Models/Overview_ImageBind.png" alt="ImageBind 模型架构。">
</div>

ImageBind 利用图像和其他模态之间的配对数据，整合多种模态表示，核心围绕图像数据。

**主要特点**

ImageBind 通过利用图像和其他模态的配对，统一了多种模态。通过采用 *InfoNCE* 作为损失函数，模型能够对各种输入之间的表示进行对齐。即使在缺乏非图像模态配对数据的情况下，ImageBind 也能有效地执行跨模态检索和零样本任务。
此外，与其他模型相比，ImageBind 的训练过程相对简单，可以通过多种方式实现。

**为何重要**

ImageBind 的主要贡献在于它能够整合多种模态，而无需特定的模态配对数据集。以图像为参考，ImageBind 将多达六种不同的模态——例如音频、文本、深度等——对齐并结合成一个统一的表示空间。其重要性在于它能够同时跨多个模态实现这种对齐，而无需为每种组合提供直接的配对，这使得它在多模态学习中具有高度的效率。

## 结论

我们简要探讨了视频中的不同模态，并深入研究了将视觉信息与其他模态整合的模型。
随着时间的推移，越来越多的研究专注于同时整合各种模态。

我很期待未来将出现的模型，这些模型将在视频内容中整合更多样化的模态。通过视频推动多模态表示学习的潜力似乎是无限的！