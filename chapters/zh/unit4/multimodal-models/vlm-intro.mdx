# 视觉语言模型简介

在本章中您将学习到：
- 多模态的简要介绍
- 视觉语言模型的介绍
- 各种学习策略
- VLM 常用的数据集
- 下游任务与评估

## 我们的世界是多模态的

人类通过多种感官探索世界：视觉、听觉、触觉和嗅觉。通过协调这些不同模态的洞察，我们才能全面理解周围的环境。最早在数学中将模态描述为不同的“峰”，但我们可以更诗意地描述模态：“每个模态各具角色，汇聚成我们理解的光谱，感官的交响曲，和谐的交融，在感知的舞蹈中，我们的世界超越了自我。”为了构建能够理解世界的 AI，机器学习领域致力于开发可以处理并整合多模态数据的模型。然而，仍有许多挑战亟待解决，包括表示和对齐。表示探索有效总结多模态数据的技术，捕捉不同模态元素之间复杂的联系。对齐则专注于识别所有元素间的联系与互动。

此外，需要认识到处理多模态的固有困难：
- 单一模态可能会主导其他模态。
- 额外的模态可能引入噪音。
- 无法保证完全覆盖所有模态。
- 不同模态之间关系复杂。

尽管面临这些挑战，机器学习社区在开发这些系统方面已取得显著进展。本章将探讨视觉与语言的融合，进而产生视觉语言模型（Vision Language Models, VLMs）。如需深入了解多模态，请参阅本单元的前一部分。

## 简介

生成图像文本描述（如图像字幕生成和视觉问答）已被研究多年，包括自动驾驶、遥感等领域。我们也见证了从传统的机器学习/深度学习训练方式向新学习范式的转变，包括预训练、微调和预测，这种方法比传统方式（如需收集大量数据等）更具优势。

这种范式首次在 [ULMFIT 论文](https://aclanthology.org/P18-1031.pdf)中提出，包括：
- 使用大量训练数据对模型进行预训练。
- 使用特定任务数据对预训练模型进行微调。
- 利用训练好的模型进行分类等下游任务。

在 VLMs 中，我们应用了这种范式并扩展到视觉图像的结合，从而达到预期的效果。例如，2021年，OpenAI 发表了一篇具有突破性意义的论文 [CLIP](https://openai.com/research/clip)，极大地推动了这一方法的采用。CLIP 使用图像-文本对比目标，通过将成对的图像和文本拉近，并在嵌入空间中将其他图像和文本推远来进行学习。这种预训练方法使 VLMs 能够捕捉丰富的视觉-语言对应知识，通过匹配任意图像和文本的嵌入，实现零样本预测。值得注意的是，CLIP 在非分布任务上表现优于 Imagenet 模型。有关 CLIP 和类似模型的进一步探讨请参见本章的下一部分！

## 机制

为了实现视觉语言模型（VLMs）的功能，需要对文本和图像进行有效结合，实现联合学习。如何做到这一点？一种简单/常见的方法是利用图像-文本对：
- 使用文本和图像编码器提取图像和文本特征。图像可以使用 **CNN** 或 **transformer** 结构。
- 通过特定的预训练目标学习视觉-语言相关性。
    - 预训练目标可以分为三组：
        - **对比**目标：通过将成对样本拉近，将其他样本推远，使 VLMs 学习判别性表示。
        - **生成**目标：通过训练网络生成图像/文本数据，使 VLMs 学习语义特征。
        - **对齐**目标：通过全局图像-文本匹配或局部区域-单词匹配对图像-文本对进行嵌入空间对齐。
- 利用学到的视觉-语言相关性，VLMs 可以通过匹配任意图像和文本的嵌入，在零样本模式下对未见数据进行评估。

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/99ac107ade7fb89aae792f3655341528e64e1fbb/clip_paper.png" alt="CLIP模型基本机制">
<p>图1：CLIP模型基本机制</p>
</div>

现有研究主要从以下三个关键角度提升 VLMs：
- 收集大规模的、有信息量的图像-文本数据。
- 设计有效的模型以从大数据中学习。
- 设计新的预训练方法/目标，以学习有效的视觉-语言相关性。

VLM 的预训练旨在预训练 VLM 以学习图像-文本相关性，以便在视觉识别任务（如分割、分类等）上进行有效的零样本预测。

## 策略

我们可以根据利用两种学习模式的方式[对 VLMs 进行分组](https://lilianweng.github.io/posts/2022-06-09-vlm/#no-training)。

- 将图像转化为可以与文本嵌入共同训练的嵌入特征。
    - 在这种方法中，我们通过将图像视为普通文本 token，将视觉信息融入语言模型，并对文本和图像的联合表示序列进行训练。具体来说，图像被分为多个较小的 patch，每个 patch 在输入序列中视为一个“token”。例如：[VisualBERT](https://arxiv.org/abs/1908.03557)，[SimVLM](https://arxiv.org/abs/2108.10904)。

- 学习可以作为冻结的、预训练语言模型前缀的优质图像嵌入。
    - 在这种方法中，我们在适应视觉信号时不改变语言模型的参数，而是为图像学习一种嵌入空间，使其与语言模型兼容。例如：[Frozen](https://arxiv.org/abs/2106.13884)，[ClipCap](https://arxiv.org/abs/2111.09734)。

- 使用专门设计的交叉注意机制，将视觉信息融合到语言模型的各层中。
    - 为了在语言模型层中增强视觉信息的融合，使用量身定制的交叉注意融合机制平衡文本生成和视觉输入。例如：[VisualGPT](https://arxiv.org/abs/2102.10407)。

- 在不进行任何训练的情况下组合视觉和语言模型。
    - 最新方法如 [MAGiC](https://arxiv.org/abs/2205.02655) 通过引导解码来生成下一个 token，无需微调。

## 常见 VLM 数据集

以下是一些在 VLM 中常用的数据集，来自 [HF 数据集](https://huggingface.co/docs/datasets/index)。

**[MSCOCO](https://huggingface.co/datasets/HuggingFaceM4/COCO)** 包含 328K 张图像，每张图像配有 5 条独立的描述。

**[NoCaps](https://huggingface.co/datasets/HuggingFaceM4/NoCaps)** 设计用于衡量对未见类别和概念的泛化能力，包含只描绘 COCO 类别的域内图像，既包含 COCO 类别又包含新类别的近域图像，以及只包含新类别的域外图像。

**[Conceptual Captions](https://huggingface.co/datasets/conceptual_captions)** 包含 300 万对图像和描述，从网络中挖掘并后处理。

**[ALIGN](https://huggingface.co/blog/vit-align)** 是一个包含十亿多对图像和文本的噪声数据集，从 Conceptual Captions 数据集中获取，无需昂贵的过滤或后处理步骤。

**[LAION](https://huggingface.co/collections/laion/openclip-datacomp-64fcac9eb961d0d12cb30bc3)** 数据集包含图像-文本对。图像-文本对从 Common Crawl 网络数据中提取，自 2014 年至 2021 年从随机网页抓取。该数据集用于训练稳定扩散模型。

## 下游任务和评估

VLMs 在许多下游任务上表现良好，包括图像分类、物体检测、语义分割、图像-文本检索和动作识别，甚至超越了传统训练的模型。

通常，VLMs 的评估设置为零样本预测和线性探测。零样本预测是最常用的评估方法，我们直接将预训练的 VLMs 应用于下游任务，无需特定任务的微调。

在线性探测中，我们冻结预训练的 VLM，并训练一个线性分类器，将 VLM 编码的嵌入进行分类，以衡量其表示能力。如何评估这些模型？我们可以检查它们在数据集上的表现，例如，

给定图像和问题，任务是正确回答问题！我们还可以检查这些模型如何回答视觉数据的问题。为此，最常用的数据集是 [CLEVR](https://cs.stanford.edu/people/jcjohns/clevr/)。

标准数据集如 MSCOCO 可能因其分布而相对简单，不能充分展示模型在更具挑战性或多样化数据集上泛化的能力。为解决这一问题，设计了如 [Hateful Memes](https://arxiv.org/abs/2005.04790) 的数据集，通过在数据集中增加难度较大的例子（“良性混淆项”）来评估模型的能力，并显示出多模态预训练的缺陷与人类表现之间存在巨大差距。

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/99ac107ade7fb89aae792f3655341528e64e1fbb/winogrand_paper.png" alt="Winogrand的想法">
<p>图2：Winogrand的想法</p>
</div>

另一个类似的数据集 **Winoground** 设计用于评估 CLIP 的实际能力。**上图**该数据集提醒我们考虑模型是否尽管取得了令人印象深刻的结果，是否真正理解了组合关系，或者只是对数据进行了泛化。例如，早期的稳定扩散和其他文本到图像模型未能清晰地计数手指。这表明，要让 VLMs 达到更高的水平，仍有很多出色的工作要完成！

## 未来展望

整个社区在快速前进，已经可以看到许多令人惊叹的工作，例如 [FLAVA](https://arxiv.org/abs/2112.04482)，尝试创建单一的“基础”模型，涵盖所有目标模态。这是未来的一种可能场景——模态无关的基础模型，可以读取和生成多种模态！但或许我们也会看到其他替代方案的发展，可以确定的是：未来充满了令人期待的可能性。

如需了解更多这些最新进展，欢迎关注 HF 的 [Transformers 库](https://huggingface.co/docs/transformers/index) 和 [Diffusers 库](https://huggingface.co/docs/diffusers/index)，我们尽快添加最新的进展和模型！如果您认为我们遗漏了重要内容，也可以为这些库提出问题并贡献代码。