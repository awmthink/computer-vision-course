# 模型优化工具和框架

## TensorFlow 模型优化工具包 (TMO)

### 概述

TensorFlow 模型优化工具包是一组用于优化机器学习模型以便部署的工具。TensorFlow Lite 的训练后量化工具可以将权重转换为 8 位精度，从而将训练模型的大小减少约 4 倍。该工具包还包括在训练期间进行剪枝和量化的 API，如果训练后量化不足以满足需求，这些工具可以帮助用户减少延迟和推理成本，将模型部署到资源受限的边缘设备，并为现有硬件或新型专用加速器优化执行。

### 设置指南

TensorFlow 模型优化工具包作为 pip 包提供，包名为 `tensorflow-model-optimization`。要安装该包，请运行以下命令：
```
pip install -U tensorflow-model-optimization
```

### 实践指南

有关如何使用 TensorFlow 模型优化工具包的实践指南，请参阅此 [notebook](https://github.com/johko/computer-vision-course/blob/main/notebooks/Unit%209%20-%20Model%20Optimization/tmo.ipynb)。

## PyTorch 量化

### 概述

为了优化模型，PyTorch 支持与典型 FP32 模型相比的 INT8 量化，从而使模型大小减少 4 倍，并减少 4 倍的内存带宽需求。PyTorch 支持多种量化深度学习模型的方法，包括以下几种：
1. 在 FP32 下训练模型，然后将模型转换为 INT8。
2. 量化感知训练，通过前向和后向传播中使用伪量化模块来处理模型的量化误差。
3. 表示量化张量并进行操作。可以直接构建部分或全部计算在低精度下进行的模型。

有关 PyTorch 中量化的更多详细信息，请参见[此处](https://pytorch.org/docs/stable/quantization.html)。

### 设置指南

PyTorch 量化在 PyTorch 包中提供为 API。使用时，只需安装 PyTorch 并导入量化 API：
```
pip install torch
import torch.quantization
```

## 实践指南

有关如何使用 PyTorch 量化的实践指南，请参阅此 [notebook](https://github.com/johko/computer-vision-course/blob/main/notebooks/Unit%209%20-%20Model%20Optimization/torch.ipynb)。

## ONNX Runtime

### 概述

ONNX Runtime 是一个跨平台的机器学习模型加速器，具有灵活的接口来集成特定硬件库。ONNX Runtime 可以与来自 PyTorch、TensorFlow/Keras、TFLite、scikit-learn 和其他框架的模型一起使用。使用 ONNX Runtime 进行推理的好处如下：
- 提高各种机器学习模型的推理性能。
- 在不同的硬件和操作系统上运行。
- 在 Python 中训练，但在 C#/C++/Java 应用中部署。
- 训练和推理使用不同框架创建的模型。

有关 ONNX Runtime 的更多详细信息，请参见[此处](https://onnxruntime.ai/docs/)。

### 设置指南

ONNX Runtime 有两个 Python 包，并且同一环境中一次只能安装其中一个包。如果需要使用 GPU 支持 ONNX Runtime，请使用 GPU 包。ONNX Runtime 的 Python 包可通过 pip 安装。安装命令如下：
```
pip install onnxruntime
```

对于 GPU 版本，请运行以下命令：
```
pip install onnxruntime-gpu
```

### 实践指南

有关如何使用 ONNX Runtime 的实践指南，请参阅此 [notebook](https://github.com/johko/computer-vision-course/blob/main/notebooks/Unit%209%20-%20Model%20Optimization/onnx.ipynb)。

## TensorRT

### 概述

NVIDIA® TensorRT™ 是一个 SDK，用于优化训练好的深度学习模型以实现高性能推理。TensorRT 包含一个用于训练深度学习模型的推理优化器和一个运行时执行模块。在用户选择的框架中训练好深度学习模型后，TensorRT 可以帮助用户实现更高的吞吐量和更低的延迟。

### 设置指南

TensorRT 作为 pip 包提供，包名为 `tensorrt`。要安装该包，请运行以下命令：
```
pip install tensorrt
```

有关其他安装方法，请参见[此处](https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html#install)。

### 实践指南

有关如何使用 TensorRT 的实践指南，请参阅此 [notebook](https://github.com/johko/computer-vision-course/blob/main/notebooks/Unit%209%20-%20Model%20Optimization/tensorrt.ipynb)。

## OpenVINO

### 概述

OpenVINO™ 工具包使用户能够优化来自几乎任何框架的深度学习模型，并在一系列 Intel® 处理器和其他硬件平台上以最佳性能部署。使用 OpenVINO 的好处包括：
- 可以直接与 OpenVINO Runtime 链接在本地运行推理，或使用 OpenVINO Model Server 从独立服务器或 Kubernetes 环境提供模型推理服务。
- 一次编写应用程序，可以在任何首选设备、语言和操作系统上部署。
- 依赖外部资源较少。
- 通过在初次推理时使用 CPU 来减少首次推理的延迟，然后在模型编译并加载到内存后切换到另一设备。

### 设置指南

OpenVINO 作为 pip 包提供，包名为 `openvino`。要安装该包，请运行以下命令：
```
pip install openvino
```

有关其他安装方法，请参见[此处](https://docs.openvino.ai/2023.2/openvino_docs_install_guides_overview.html?VERSION=v_2023_2_0&OP_SYSTEM=LINUX&DISTRIBUTION=ARCHIVE)。

### 实践指南

有关如何使用 OpenVINO 的实践指南，请参阅此 [notebook](https://github.com/johko/computer-vision-course/blob/main/notebooks/Unit%209%20-%20Model%20Optimization/openvino.ipynb)。

## Optimum

### 概述

Optimum 是 [Transformers](https://huggingface.co/docs/transformers) 的扩展，提供一组工具，用于在特定硬件上优化训练和运行模型的性能，以确保最大效率。在迅速发展的 AI 领域，不断出现专门的硬件和独特的优化。Optimum 让开发人员能够轻松利用这些多样化的平台，同时保持 Transformers 的易用性。目前，Optimum 支持的平台有：
1. [Habana](https://huggingface.co/docs/optimum/habana/index)
2. [Intel](https://huggingface.co/docs/optimum/intel/index)
3. [Nvidia](https://github.com/huggingface/optimum-nvidia)
4. [AWS Trainium 和 Inferentia](https://huggingface.co/docs/optimum-neuron/index)
5. [AMD](https://huggingface.co/docs/optimum/amd/index)
6. [FuriosaAI](https://huggingface.co/docs/optimum/furiosa/index)
7. [ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/overview)
8. [BetterTransformer](https://huggingface.co/docs/optimum/bettertransformer/overview)

### 设置指南

Optimum 作为 pip 包提供，包名为 `optimum`。要安装该包，请运行以下命令：
```
pip install optimum
```

有关加速器特定功能的安装，请参见[此处](https://huggingface.co/docs/optimum/installation)。

### 实践指南

有关如何使用 Optimum 进行量化的实践指南，请参阅此 [notebook](https://github.com/johko/computer-vision-course/blob/main/notebooks/Unit%209%20-%20Model%20Optimization/optimum.ipynb)。

## EdgeTPU

### 概述

Edge TPU 是 Google 为边缘运行 AI 而专门设计的 ASIC，它在物理和功耗占用较小的情况下提供高性能，使高精度 AI 可以在边缘部署。使用 EdgeTPU 的好处包括：
- 补充了 Cloud TPU 和 Google Cloud 服务，提供从云到边缘的硬件+软件基础设施，以部署基于 AI 的解决方案。
- 在较小的物理和功率占用下实现高性能。
- 结合定制硬件、开放软件和最先进的 AI 算法，为边缘提供高质量、易于部署的 AI 解决方案。

有关 EdgeTPU 的更多详细信息，请参见[此处](https://cloud.google.com/edge-tpu)。

有关如何设置和使用 EdgeTPU 的指南，请参阅此 [notebook](https://github.com/johko/computer-vision-course/blob/main/notebooks/Unit%209%20-%20Model%20Optimization/edge_tpu.ipynb)。